{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hannover, braunschweig, wolfsburg\n",
    "source_city = 'hannover'\n",
    "\n",
    "# Load data\n",
    "feats_mx = np.load(f'../data/feats_mx2_{source_city}.npy')\n",
    "labels = pd.read_csv(f'../data/labels_{source_city}.csv')\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.get_device_name()\n",
    "    \n",
    "embedding_dims = 256\n",
    "batch_size = 32\n",
    "epochs = 300\n",
    "k_avg_streets = 10\n",
    "\n",
    "# Triplet Loss Margin\n",
    "margin = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Cluster Size: {labels['cluster'].max()+1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats_mx[..., 1:])\n",
    "feats_mx[..., 1:] = scaler.transform(feats_mx[..., 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feats_mx[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge them to one table\n",
    "# Transform np array to df and merge with labels\n",
    "feats_df = pd.DataFrame(data=feats_mx)\n",
    "feats_df = feats_df.rename(columns={feats_df.columns[0]: 'id'})\n",
    "# join inner (we have less labels than street feats)\n",
    "train_df = pd.merge(labels, feats_df, on=\"id\")\n",
    "# drop id column\n",
    "train_df = train_df.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create Dataset Class ######\n",
    "\n",
    "class RoadData(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        self.is_train = train\n",
    "        \n",
    "        if self.is_train:            \n",
    "            self.feats = df.iloc[:, 1:].values.astype(np.float32)\n",
    "            self.labels = df.iloc[:, 0].values\n",
    "            self.index = df.index.values\n",
    "        else:\n",
    "            self.feats = df.values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feats)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        anchor = self.feats[item]\n",
    "        \n",
    "        if self.is_train:\n",
    "            anchor_label = self.labels[item]\n",
    "\n",
    "            positive_list = self.index[self.index!=item][self.labels[self.index!=item]==anchor_label]\n",
    "\n",
    "            positive_item = random.choice(positive_list)\n",
    "            positive = self.feats[positive_item]\n",
    "            \n",
    "            negative_list = self.index[self.index!=item][self.labels[self.index!=item]!=anchor_label]\n",
    "            negative_item = random.choice(negative_list)\n",
    "            negative = self.feats[negative_item]\n",
    "            \n",
    "            return anchor, positive, negative, anchor_label\n",
    "        \n",
    "        else:\n",
    "            return anchor\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers have shown that selcting randomly the positive and negative samples is not the best approach. So possible to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initialize dataset and dataloader ####\n",
    "train_ds = RoadData(train_df, train=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Implementation of Model and Loss ######\n",
    "\n",
    "# Model quite simple. Try different ones?\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super(TripletNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(16, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU()\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        self.fc2 = nn.Sequential(nn.Linear(emb_dim+8, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim)\n",
    "                                )\n",
    "        \n",
    "        self.ge_extraction = nn.Sequential(nn.Linear(128, 64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(64, 32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(32, 16),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(16, 8),\n",
    "                                nn.ReLU()\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "                                nn.Linear(90, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(emb_dim, emb_dim)\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #streat_feats = x[:,:16]\n",
    "        #streat_feats = self.fc1(streat_feats)\n",
    "        #ge = x[:,16:]\n",
    "        #ge = self.ge_extraction(ge)\n",
    "        #x = torch.cat((streat_feats, ge), dim=1)\n",
    "        #x = self.fc2(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def calc_euclidean(self, x1, x2):\n",
    "        return (x1 - x2).pow(2).sum(1)\n",
    "    \n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        distance_positive = self.calc_euclidean(anchor, positive)\n",
    "        distance_negative = self.calc_euclidean(anchor, negative)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init instances ###\n",
    "model = TripletNet(embedding_dims)\n",
    "#model.apply(init_weights)\n",
    "\n",
    "# JIT Script for performance\n",
    "#model = torch.jit.script(model).to(device)#\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#criterion = torch.jit.script(TripletLoss(margin))\n",
    "criterion = TripletLoss(margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### TRAINING ####\n",
    "model.train()\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    running_loss = []\n",
    "    for step, (anchor, positive, negative, anchor_label) in enumerate(tqdm(train_loader, desc=\"Training\", leave=False)):\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        anchor_out = model(anchor)\n",
    "        positive_out = model(positive)\n",
    "        negative_out = model(negative)\n",
    "        \n",
    "        loss = criterion(anchor_out, positive_out, negative_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss.append(loss.cpu().detach().numpy())\n",
    "    print(\"Epoch: {}/{} - Loss: {:.4f}\".format(epoch+1, epochs, np.mean(running_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Params ###\n",
    "#torch.save(model.state_dict(), \"trained_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "\n",
    "We need to evaluate the model. We do this by application.\n",
    "\n",
    "For every target street, we select the closest n source streets and calculate the missmatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hannover, braunschweig, wolfsburg\n",
    "source_city = 'hannover'\n",
    "target_city = 'braunschweig'\n",
    "\n",
    "train_date_begin = \"2019-01-10\"\n",
    "train_date_end = \"2019-01-31\"\n",
    "hour_from = \"7\"\n",
    "hour_to = \"21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Road Features for Source and Target city\n",
    "# Load data\n",
    "source_feats_mx = np.load(f'../data/feats_mx2_{source_city}.npy')\n",
    "target_feats_mx = np.load(f'../data/feats_mx2_{target_city}.npy')\n",
    "# Sort them (when we create gwn input its also sorted)\n",
    "source_feats_mx = source_feats_mx[np.argsort(source_feats_mx[:, 0])]\n",
    "target_feats_mx = target_feats_mx[np.argsort(target_feats_mx[:, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standerdize\n",
    "source_feats_mx[..., 1:] = scaler.transform(source_feats_mx[..., 1:])\n",
    "target_feats_mx[..., 1:] = scaler.transform(target_feats_mx[..., 1:])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the roads, which have historical speed data ###\n",
    "\n",
    "# This cell filters out those roads on the source which have bad data quality\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(sys.path[0], '..'))\n",
    "\n",
    "import yaml\n",
    "from src import db_requests\n",
    "from src import data_preperation\n",
    "\n",
    "\n",
    "with open(\"../db.yaml\", 'r') as dbfi:\n",
    "    db_credentials = yaml.safe_load(dbfi)\n",
    "\n",
    "min_measurements = 3000\n",
    "\n",
    "source_hist_data = db_requests.getTrafficDataMinMeasurements(source_city, min_measurements, train_date_begin, train_date_end, hour_from, hour_to, db_credentials=db_credentials)\n",
    "# Get the ids which are in the data\n",
    "source_ids = source_hist_data['id'].unique()\n",
    "# For the Feature Mx filter out those that are not in the data\n",
    "mask = np.isin(source_feats_mx[:,0],source_ids)\n",
    "source_feats_mx = source_feats_mx[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate idx to id mapping and road feats\n",
    "source_idx2id = source_feats_mx[:,0]\n",
    "source_feats_mx = source_feats_mx[:,1:]\n",
    "\n",
    "\n",
    "target_idx2id = target_feats_mx[:,0]\n",
    "target_feats_mx = target_feats_mx[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_mx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_feats_mx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SOURCE EMBEDDINGS #######\n",
    "# Use DataLoader\n",
    "source_feats_df = pd.DataFrame(data=source_feats_mx)\n",
    "#source_feats_df = source_feats_df.drop(columns={source_feats_df.columns[0]}) # ether drop at mx or here\n",
    "\n",
    "source_feats_ds = RoadData(source_feats_df, train=False)\n",
    "source_loader = DataLoader(source_feats_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Run Embedding Model\n",
    "source_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(source_loader): # Here should be the test set\n",
    "        source_embeddings.append(model(data.to(device)).cpu().numpy())\n",
    "        \n",
    "source_embeddings = np.concatenate(source_embeddings)\n",
    "source_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TARGET EMBEDDINGS #######\n",
    "# Use DataLoader\n",
    "target_feats_df = pd.DataFrame(data=target_feats_mx)\n",
    "#source_feats_df = source_feats_df.drop(columns={source_feats_df.columns[0]}) # ether drop at mx or here\n",
    "\n",
    "target_feats_ds = RoadData(target_feats_df, train=False)\n",
    "target_loader = DataLoader(target_feats_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Run Embedding Model\n",
    "target_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(target_loader): # Here should be the test set\n",
    "        target_embeddings.append(model(data.to(device)).cpu().numpy())\n",
    "        \n",
    "target_embeddings = np.concatenate(target_embeddings)\n",
    "target_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the indices of closest k Vectors (not sorted!)\n",
    "def closest_vector_filtered(b,A, sl_filter, k=1):\n",
    "    subs = (b[None,:] - A)\n",
    "    sq_dist = np.einsum('ij,ij->i',subs, subs)\n",
    "    # As we only want embeddings with same SL,\n",
    "    # we set all distances, which have a different speed limit to 1000\n",
    "    sq_dist[sl_filter] = 10000000\n",
    "    return np.argpartition(sq_dist, k)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to find for each target street closest k source streets\n",
    "k = k_avg_streets\n",
    "mapping = {}\n",
    "for i in range(target_embeddings.shape[0]):\n",
    "    # Get embedding\n",
    "    emb_i = target_embeddings[i]\n",
    "    # Get speedlimit\n",
    "    sl_i = target_feats_mx[i,1]\n",
    "    # Get all indices from source with same speedlimit\n",
    "    indices = source_feats_mx[:,1] == target_feats_mx[i,1]\n",
    "    sl_filter = np.invert(indices) \n",
    "    idx = closest_vector_filtered(emb_i,source_embeddings, sl_filter, k=k)\n",
    "    mapping[i] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID to ID mapping\n",
    "id_mapping = {}\n",
    "for key, value in mapping.items():\n",
    "    new_key = int(target_idx2id[key])\n",
    "    #new_value = int(source_idx2id[value])\n",
    "    new_value = [int(source_idx2id[x]) for x in value]\n",
    "    \n",
    "    id_mapping[new_key] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FETCH DATA #####\n",
    "\n",
    "# load traffic speed data\n",
    "source_data = db_requests.getTrafficDataMinMeasurements(source_city, min_measurements, train_date_begin, train_date_end, hour_from, hour_to, db_credentials=db_credentials)\n",
    "target_data = db_requests.getTrafficDataMinMeasurements(target_city, min_measurements, train_date_begin, train_date_end, hour_from, hour_to, db_credentials=db_credentials)\n",
    "\n",
    "# Get Street Features: id, length, speed_limit as max_speed, type, source, target\n",
    "streetFeats = db_requests.getStreetGraph(db_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCRNN Function needs a format like this:\n",
    "#     id1  id2  id3\n",
    "# t1\n",
    "# t2\n",
    "# t3\n",
    "source_mx = pd.pivot_table(source_data, values='speed', index='time', columns=['id'], aggfunc=np.mean)\n",
    "target_mx = pd.pivot_table(target_data, values='speed', index='time', columns=['id'], aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_mx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through columns\n",
    "# find matching street in source\n",
    "# fill column with source street values\n",
    "# NEW: Aggregate different streets then fill\n",
    "target_source_mx = pd.DataFrame(index=target_mx.index, columns=target_mx.columns)\n",
    "for road_id in target_mx.columns:\n",
    "    matched_source_id = id_mapping[road_id]\n",
    "    #print(matched_source_id)\n",
    "    target_source_mx[road_id] = source_mx[matched_source_id].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_source_mx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an Error Value\n",
    "T = target_mx.values\n",
    "T_new = target_source_mx.values\n",
    "\n",
    "# Fill NANs\n",
    "T[np.isnan(T)] = 0\n",
    "T_new[np.isnan(T_new)] = 0\n",
    "\n",
    "\n",
    "diff_mx = target_mx.values - target_source_mx.values\n",
    "#diff_mx = target_mx.values[382:] - target_source_mx.values[:-382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_cols = np.mean(np.abs(diff_mx), axis = 0)\n",
    "MAE_cols[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAE_cols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAE = np.mean(MAE_cols)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 13.760675093470507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(np.square(diff_mx), axis = 0))\n",
    "np.mean(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 14.630686162941847\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAFE MAPPING FOR GWN MODEL \n",
    "safe = False\n",
    "if safe:\n",
    "    import json\n",
    "    mapping_save_path = f'../data/mapping_{source_city}_{target_city}.json'\n",
    "    # Save mapping\n",
    "    with open(mapping_save_path, 'w') as f:\n",
    "        json.dump(id_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.479080117425092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into bigger Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.where(MAE_cols>50)[0]\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_idx = idxs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get id\n",
    "target_mx.columns[first_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mx.iloc[:,first_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_source_mx.iloc[:,first_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_id = target_mx.columns[first_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_source_id = id_mapping[first_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "streetFeats.loc[streetFeats['id']==first_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streetFeats.loc[streetFeats['id'].isin(first_source_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
