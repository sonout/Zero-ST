{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input data for GraphWaveNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schestakov/.local/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# We want to import stuff from parent directory\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from src import db_requests\n",
    "from src import data_preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic speed input data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFIGURATIONS #####\n",
    "\n",
    "with open(\"../db.yaml\", 'r') as dbfi:\n",
    "    db_credentials = yaml.safe_load(dbfi)\n",
    "\n",
    "## DATA ##\n",
    "source_city = \"wolfsburg\"\n",
    "target_city = \"braunschweig\"\n",
    "\n",
    "# Note: we get all data and split later\n",
    "train_date_begin = \"2018-12-01\"\n",
    "train_date_end = \"2019-01-31\"\n",
    "test_date_begin = \"2018-12-01\"\n",
    "test_date_end = \"2019-01-31\"\n",
    "hour_from = \"7\"\n",
    "hour_to = \"21\"\n",
    "\n",
    "min_measurements = 3000\n",
    "\n",
    "mapping_save_path = f'../data/mapping_{source_city}_{target_city}.json'\n",
    "\n",
    "output_dir = f\"data/zero_st2/{source_city}_{target_city}\"\n",
    "output_adj_filename = f\"data/sensor_graph/adj_mx_{target_city}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Ablation/mapping_notopology_wolfsburg_braunschweig.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e86df7b995c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### Load target_source_matching ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Open Mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping_save_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mid_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# keys are imported as str. Convert to int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Ablation/mapping_notopology_wolfsburg_braunschweig.json'"
     ]
    }
   ],
   "source": [
    "#### Load target_source_matching ####\n",
    "# Open Mapping\n",
    "with open(mapping_save_path) as f:\n",
    "    id_mapping = json.load(f)\n",
    "# keys are imported as str. Convert to int\n",
    "id_mapping = dict([int(k), v] for k, v in id_mapping.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FETCH DATA #####\n",
    "\n",
    "# load traffic speed data\n",
    "source_data = db_requests.getTrafficDataMinMeasurements(source_city, min_measurements, train_date_begin, train_date_end, hour_from, hour_to, db_credentials=db_credentials)\n",
    "target_data = db_requests.getTrafficDataMinMeasurements(target_city, min_measurements, train_date_begin, train_date_end, hour_from, hour_to, db_credentials=db_credentials)\n",
    "\n",
    "# Get Street Features: id, length, speed_limit as max_speed, type, source, target\n",
    "streetFeats = db_requests.getStreetGraph(db_credentials)\n",
    "\n",
    "# Merge with traffic speed data\n",
    "#source_data = source_data.merge(streetFeats, left_on=\"id\", right_on=\"id\")\n",
    "#test_data = test_data.merge(streetFeats, left_on=\"id\", right_on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = train_data.values\n",
    "#np.savez_compressed(\"data/mobility_data\", a = a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCRNN Function needs a format like this:\n",
    "#     id1  id2  id3\n",
    "# t1\n",
    "# t2\n",
    "# t3\n",
    "source_mx = pd.pivot_table(source_data, values='speed', index='time', columns=['id'], aggfunc=np.mean)\n",
    "target_mx = pd.pivot_table(target_data, values='speed', index='time', columns=['id'], aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "source_mx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through columns\n",
    "# find matching street in source\n",
    "# fill column with source street values\n",
    "# NEW: Aggregate different streets then fill\n",
    "target_source_mx = pd.DataFrame(index=target_mx.index, columns=target_mx.columns)\n",
    "for road_id in target_mx.columns:\n",
    "    matched_source_id = id_mapping[road_id]\n",
    "    target_source_mx[road_id] = source_mx[matched_source_id].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_source_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an Error Value\n",
    "T = target_mx.values\n",
    "T_new = target_source_mx.values\n",
    "# Fill NANs\n",
    "T[np.isnan(T)] = 0\n",
    "T_new[np.isnan(T_new)] = 0\n",
    "diff_mx = target_mx.values - target_source_mx.values\n",
    "np.set_printoptions(precision=3)\n",
    "MAE_cols = np.mean(np.abs(diff_mx), axis = 0)\n",
    "MAE = np.mean(MAE_cols)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this function creates: <br>\n",
    "X: (N, timesteps, nodes, (speed, timeofday))<br>\n",
    "Y: (N, timesteps, nodes, (speed, timeofday))\n",
    "<br>\n",
    "We want:<br>\n",
    "X: (N, timesteps, nodes, (daily periodic, weekly periodic))<br>\n",
    "Y: (N, timesteps=1, nodes, speed)\n",
    "<br>\n",
    "So we need to get, instead of the last 12 timesteps: <br>\n",
    "4 timesteps from the last 3 days at the same time + <br>\n",
    "4 timesteps from the last 3 weeks at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function taken from DCRNN\n",
    "def generate_train_val_test_periodic(df, test_share = 0.2, train_share = 0.7):\n",
    "    if df.isnull().values.any():\n",
    "        df = df.fillna(0)\n",
    "    print('Warning: NaN Values in Data found. Filled them with 0!')\n",
    "\n",
    "    # 0 is the latest observed sample.\n",
    "    x_offsets = np.sort(np.arange(-2, 2, 1))\n",
    "    # Predict the next one hour\n",
    "    y_offsets = np.sort(np.arange(0, 1, 1)) # = [0] -> we dont use offsets for y just predict t\n",
    "    \n",
    "    # x: (num_samples, input_length, num_nodes, input_dim)\n",
    "    # y: (num_samples, output_length, num_nodes, output_dim)\n",
    "    x, y = generate_graph_seq2seq_io_data_periodic(\n",
    "        df,\n",
    "        x_offsets=x_offsets,\n",
    "        y_offsets=y_offsets,\n",
    "        add_time_in_day=False,\n",
    "        add_day_in_week=False,\n",
    "    )\n",
    "\n",
    "    # Write the data into npz file.\n",
    "    # num_test = 6831, using the last 6831 examples as testing.\n",
    "    # for the rest: 7/8 is used for training, and 1/8 is used for validation.\n",
    "    num_samples = x.shape[0]\n",
    "    # Schestakov: swaped round() to int() as it was giving an error in line 86 for not being int\n",
    "    num_test = int(num_samples * test_share)\n",
    "    num_train = int(num_samples * train_share)\n",
    "    num_val = num_samples - num_test - num_train\n",
    "\n",
    "    # train\n",
    "    x_train, y_train = x[:num_train], y[:num_train]\n",
    "    # val\n",
    "    x_val, y_val = (\n",
    "        x[num_train: num_train + num_val],\n",
    "        y[num_train: num_train + num_val],\n",
    "    )\n",
    "    # test\n",
    "    x_test, y_test = x[-num_test:], y[-num_test:]\n",
    "    \n",
    "    print(\"x_train shape: \", x_train.shape, \", y_train shape: \", y_train.shape)\n",
    "    print(\"x_val shape: \", x_val.shape, \", y_val shape: \", y_val.shape)\n",
    "    print(\"x_test shape: \", x_test.shape, \", y_test shape: \", y_test.shape)\n",
    "    \n",
    "    return x_train, y_train,  x_val, y_val, x_test, y_test, x_offsets, y_offsets\n",
    "\n",
    "\n",
    "def generate_graph_seq2seq_io_data_periodic(\n",
    "        df, x_offsets, y_offsets, add_time_in_day=True, add_day_in_week=False, scaler=None):\n",
    "    \"\"\"\n",
    "    Generate samples from\n",
    "    :param df:\n",
    "    :param x_offsets:\n",
    "    :param y_offsets:\n",
    "    :param add_time_in_day:\n",
    "    :param add_day_in_week:\n",
    "    :param scaler:\n",
    "    :return:\n",
    "    # x: (epoch_size, input_length, num_nodes, input_dim)\n",
    "    # y: (epoch_size, output_length, num_nodes, output_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples, num_nodes = df.shape\n",
    "    data = np.expand_dims(df.values, axis=-1)\n",
    "    data_list = [data]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    if add_time_in_day:\n",
    "        time_ind = (df.index.values - df.index.values.astype(\"datetime64[D]\")) / np.timedelta64(1, \"D\")\n",
    "        time_in_day = np.tile(time_ind, [1, num_nodes, 1]).transpose((2, 1, 0))\n",
    "        data_list.append(time_in_day)\n",
    "    if add_day_in_week:\n",
    "        day_in_week = np.zeros(shape=(num_samples, num_nodes, 7))\n",
    "        day_in_week[np.arange(num_samples), :, df.index.dayofweek] = 1\n",
    "        data_list.append(day_in_week)\n",
    "\n",
    "    data = np.concatenate(data_list, axis=-1)\n",
    "\n",
    "\n",
    "    min_t = df.index.get_loc(df.index[0] + datetime.timedelta(days=21, minutes=30))\n",
    "    max_t = df.shape[0] - 3\n",
    "    x_weeks, x_days, y = [], [], []\n",
    "    for t in range(min_t, max_t):\n",
    "        time = df.index[t] \n",
    "        # Previous 3 weeks \n",
    "        w1 = df.index.get_loc(time - datetime.timedelta(days=21))\n",
    "        w2 = df.index.get_loc(time - datetime.timedelta(days=14))\n",
    "        w3 = df.index.get_loc(time - datetime.timedelta(days=7))\n",
    "        x_w1 = data[w1+x_offsets]\n",
    "        x_w2 = data[w2+x_offsets]\n",
    "        x_w3 = data[w3+x_offsets]\n",
    "\n",
    "        x_w = np.vstack((x_w1,x_w2,x_w3))\n",
    "\n",
    "        # Previous 3 days\n",
    "        d1 = df.index.get_loc(time - datetime.timedelta(days=3))\n",
    "        d2 = df.index.get_loc(time - datetime.timedelta(days=2))\n",
    "        d3 = df.index.get_loc(time - datetime.timedelta(days=1))\n",
    "\n",
    "        x_d1 = data[d1+x_offsets]\n",
    "        x_d2 = data[d2+x_offsets]\n",
    "        x_d3 = data[d3+x_offsets]\n",
    "\n",
    "        x_d = np.vstack((x_d1,x_d2,x_d3))\n",
    "\n",
    "        # Labels\n",
    "        y_t = data[t + y_offsets]\n",
    "\n",
    "\n",
    "\n",
    "        x_weeks.append(x_w)\n",
    "        x_days.append(x_d)\n",
    "        y.append(y_t)\n",
    "\n",
    "\n",
    "    x_weeks = np.stack(x_weeks, axis=0)\n",
    "    x_days = np.stack(x_days, axis=0)\n",
    "    x = np.concatenate((x_weeks,x_days),axis=3)\n",
    "    y = np.stack(y, axis=0)   \n",
    "    return x,y\n",
    "\n",
    "def store_data(output_dir, x_train, y_train,  x_val, y_val, x_test, y_test, x_offsets, y_offsets):\n",
    "    for cat in [\"train\", \"val\", \"test\"]:\n",
    "        _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "        print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "        np.savez_compressed(\n",
    "            os.path.join(output_dir, \"%s.npz\" % cat),\n",
    "            x=_x,\n",
    "            y=_y,\n",
    "            x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "            y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are missing timesteps in the data which made the dataprocessing not working.\n",
    "# We aggregate the missing timesteps.\n",
    "def aggregate_missing_timesteps(df):\n",
    "    while(check_if_missing(df)):\n",
    "        df = aggregate_missing_timesteps_helper(df)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "def aggregate_missing_timesteps_helper(df):\n",
    "    #### The dataframe has not all timesteps\n",
    "    #### Add mmissing timesteps and average the missing values\n",
    "    df2 = df.copy()\n",
    "    last_timestep = df.index[0] - datetime.timedelta(hours=10, minutes=15)\n",
    "    for index, row in df.iterrows(): \n",
    "        timestep_minus_15 = index - datetime.timedelta(minutes=15)\n",
    "        # Check first if its outside the range 7:00 - 21:00\n",
    "        if not datetime.datetime.time(timestep_minus_15) < datetime.time(hour=7):\n",
    "\n",
    "            # Now check if it is the same as last timestep\n",
    "            if not timestep_minus_15 == last_timestep:\n",
    "                # change index of row\n",
    "                missing_row = row\n",
    "                missing_row.name = timestep_minus_15\n",
    "                df2 = df2.append(missing_row)\n",
    "        else:\n",
    "            # If its earlier than 7, check if last timestamp of 21:00 is available\n",
    "            timestep_last_day = index - datetime.timedelta(hours=10, minutes=15)\n",
    "            if not last_timestep == timestep_last_day:\n",
    "                missing_row = row\n",
    "                missing_row.name = timestep_last_day\n",
    "                df2 = df2.append(missing_row)\n",
    "                \n",
    "\n",
    "        last_timestep = index\n",
    "    df2 = df2.sort_index()\n",
    "    return df2\n",
    "\n",
    "def check_if_missing(df):\n",
    "    missing = False\n",
    "    last_timestep = df.index[0] - datetime.timedelta(hours=10, minutes=15)\n",
    "    for index, row in df.iterrows(): \n",
    "        timestep_minus_15 = index - datetime.timedelta(minutes=15)\n",
    "\n",
    "        # Check first if its outside the range 7:00 - 21:00\n",
    "        if not datetime.datetime.time(timestep_minus_15) < datetime.time(hour=7):\n",
    "\n",
    "            # Now check if it is the same as last timestep\n",
    "            if not timestep_minus_15 == last_timestep:\n",
    "                missing = True\n",
    "        else:\n",
    "            # If its earlier than 7, check if last timestamp of 21:00 is available\n",
    "            timestep_last_day = index - datetime.timedelta(hours=10, minutes=15)\n",
    "            if not last_timestep == timestep_last_day:\n",
    "                missing = True\n",
    "        last_timestep = index\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_source_mx = aggregate_missing_timesteps(target_source_mx)\n",
    "target_mx = aggregate_missing_timesteps(target_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train,  x_val, y_val, x_test, y_test, x_offsets, y_offsets = generate_train_val_test_periodic(target_source_mx,  test_share= 0.243, train_share=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rl, y_train_rl,  x_val_rl, y_val_rl, x_test_rl, y_test_rl, x_offsets2, y_offsets2 = generate_train_val_test_periodic(target_mx,  test_share= 0.243, train_share=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Safe all ##########\n",
    "\n",
    "# For training we use x and y from target_source_mx\n",
    "# For testing we use x from target_source_mx, but y from target_mx to have the real numbers\n",
    "\n",
    "store_data(output_dir, x_train, y_train,  x_val, y_val, x_test, y_test_rl, x_offsets, y_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency Matrix - Data creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to get a Table with Columns: ID | ID | distance\n",
    "# This is used then to calculate the adj_mx\n",
    "\n",
    "# First get all used IDs \n",
    "id_list = list(target_mx.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### DB Functions ########################\n",
    "import sqlalchemy as sa\n",
    "\n",
    "def createEngine(db_credentials):\n",
    "    db_url = \"postgresql://\"+db_credentials[\"user\"]+\":\"+db_credentials[\"pw\"]+\"@\"\\\n",
    "             +db_credentials[\"host\"]+\"/mobility-share\"\n",
    "\n",
    "    engine = sa.create_engine(db_url)\n",
    "    return engine\n",
    "\n",
    "def mobility_share_query(db_credentials, query):\n",
    "    db_url = \"postgresql://\" + db_credentials[\"user\"] + \":\" + db_credentials[\"pw\"] + \"@\" \\\n",
    "                 + db_credentials[\"host\"] + \"/mobility-share\"\n",
    "    engine = sa.create_engine(db_url)\n",
    "    #query = \"select id, length, speed_limit as max_speed, type, source, target from streetgraph\"\n",
    "    ret = pd.read_sql(query, engine)\n",
    "    return ret\n",
    "\n",
    "def id_list_to_string(id_list):\n",
    "    ret = \"(\"\n",
    "    for _id in id_list:\n",
    "        ret = ret + \"\\'\" + str(_id) + \"\\', \"\n",
    "    ret = ret[:-2] + \")\"\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the query which returns the Distance Table with selected IDs\n",
    "query = \"SELECT sg1.id, sg2.id, ST_Distance_Sphere(st_line_interpolate_point(sg1.geometry,0.5), st_line_interpolate_point(sg2.geometry,0.5))\"\\\n",
    "        \" FROM public.streetgraph_hannover sg1, public.streetgraph_hannover sg2\"\\\n",
    "        \" WHERE sg1.id in \" + id_list_to_string(id_list) +\\\n",
    "        \" AND sg2.id in \" + id_list_to_string(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distance_df = mobility_share_query(db_credentials, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ FROM DCRNN gen_adj_mx.py ###################\n",
    "\n",
    "def get_adjacency_matrix(distance_df, sensor_ids, normalized_k=0.1):\n",
    "    \"\"\"\n",
    "\n",
    "    :param distance_df: data frame with three columns: [from, to, distance].\n",
    "    :param sensor_ids: list of sensor ids.\n",
    "    :param normalized_k: entries that become lower than normalized_k after normalization are set to zero for sparsity.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_sensors = len(sensor_ids)\n",
    "    dist_mx = np.zeros((num_sensors, num_sensors), dtype=np.float32)\n",
    "    dist_mx[:] = np.inf\n",
    "    # Builds sensor id to index map.\n",
    "    #sensor_id_to_ind = {}\n",
    "    #for i, sensor_id in enumerate(sensor_ids):\n",
    "    #    sensor_id_to_ind[sensor_id] = i\n",
    "\n",
    "    sensor_id_to_ind = dict(zip(sensor_ids.iloc[:,0], sensor_ids.index))\n",
    "\n",
    "    # Fills cells in the matrix with distances.\n",
    "    for row in distance_df.values:\n",
    "        if row[0] not in sensor_id_to_ind or row[1] not in sensor_id_to_ind:\n",
    "            continue\n",
    "        dist_mx[sensor_id_to_ind[row[0]], sensor_id_to_ind[row[1]]] = row[2]\n",
    "\n",
    "    # Calculates the standard deviation as theta.\n",
    "    distances = dist_mx[~np.isinf(dist_mx)].flatten()\n",
    "    std = distances.std()\n",
    "    adj_mx = np.exp(-np.square(dist_mx / std))\n",
    "    # Make the adjacent matrix symmetric by taking the max.\n",
    "    # adj_mx = np.maximum.reduce([adj_mx, adj_mx.T])\n",
    "\n",
    "    # Sets entries that lower than a threshold, i.e., k, to zero for sparsity.\n",
    "    adj_mx[adj_mx < normalized_k] = 0\n",
    "    return sensor_ids, sensor_id_to_ind, adj_mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df =  pd.DataFrame({'ID':id_list})\n",
    "sensor_ids, sensor_id_to_ind, adj_mx = get_adjacency_matrix(distance_df,id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle file.\n",
    "with open(output_adj_filename, 'wb') as f:\n",
    "    pickle.dump([sensor_ids, sensor_id_to_ind, adj_mx], f, protocol=2)\n",
    "print(\"Saved as \" + output_adj_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
