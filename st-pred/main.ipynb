{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import util\n",
    "import matplotlib.pyplot as plt\n",
    "from engine import trainer\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(object):\n",
    "    source_city = \"hannover\" \n",
    "    target_city = \"wolfsburg\"\n",
    "    device = 'cuda:0'\n",
    "    data = f\"data/zero_st2/{source_city}_{target_city}\" #'data/METR-LA'\n",
    "    adjdata = f\"data/sensor_graph/adj_mx_{target_city}.pkl\"\n",
    "    adjtype = 'doubletransition'\n",
    "    gcn_bool = True # 'whether to add graph convolution layer\n",
    "    aptonly = True # whether only adaptive adj\n",
    "    addaptadj = True # whether add adaptive adj\n",
    "    randomadj = True # whether random initialize adaptive adj\n",
    "    nhid = 32\n",
    "    \n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    dropout = 0.4\n",
    "    weight_decay = 0.01\n",
    "    epochs = 30\n",
    "    print_every = 50\n",
    "    save = 'save/'\n",
    "    expid = 1\n",
    "    \n",
    "    # Those variables can be extracted from dataset\n",
    "    in_dim = 2\n",
    "    num_nodes = 0 #207\n",
    "    seq_length = 12 # This is the output seq_length\n",
    "    N = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "device = torch.device(args.device)\n",
    "sensor_ids, sensor_id_to_ind, adj_mx = util.load_adj(args.adjdata,args.adjtype)\n",
    "dataloader = util.load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size)\n",
    "\n",
    "# Get Shape of Data\n",
    "args.N, _, args.num_nodes, args.in_dim  = dataloader['x_train'].shape\n",
    "_, args.seq_length,_ ,_  = dataloader['y_train'].shape\n",
    "print(\"N: \" + str(args.N))\n",
    "print(\"seq_length: \" + str(args.seq_length))\n",
    "print(\"num_nodes: \" + str(args.num_nodes))\n",
    "print(\"in_dim: \" + str(args.in_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set seed\n",
    "#torch.manual_seed(args.seed)\n",
    "#np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "scaler = dataloader['scaler']\n",
    "supports = [torch.tensor(i).to(device) for i in adj_mx]\n",
    "\n",
    "print(args)\n",
    "\n",
    "if args.randomadj:\n",
    "    adjinit = None\n",
    "else:\n",
    "    adjinit = supports[0]\n",
    "\n",
    "if args.aptonly:\n",
    "    supports = None\n",
    "\n",
    "\n",
    "\n",
    "engine = trainer(scaler, args.in_dim, args.seq_length, args.num_nodes, args.nhid, args.dropout,\n",
    "                     args.learning_rate, args.weight_decay, device, supports, args.gcn_bool, args.addaptadj,\n",
    "                     adjinit)\n",
    "\n",
    "\n",
    "\n",
    "print(\"start training...\",flush=True)\n",
    "his_loss =[]\n",
    "his_loss_train = []\n",
    "val_time = []\n",
    "train_time = []\n",
    "for i in range(1,args.epochs+1):\n",
    "    #if i % 10 == 0:\n",
    "        #lr = max(0.000002,args.learning_rate * (0.1 ** (i // 10)))\n",
    "        #for g in engine.optimizer.param_groups:\n",
    "            #g['lr'] = lr\n",
    "    train_loss = []\n",
    "    train_mape = []\n",
    "    train_rmse = []\n",
    "    t1 = time.time()\n",
    "    dataloader['train_loader'].shuffle()\n",
    "    for iter, (x, y) in enumerate(dataloader['train_loader'].get_iterator()):\n",
    "        trainx = torch.Tensor(x).to(device)\n",
    "        trainx= trainx.transpose(1, 3)\n",
    "        trainy = torch.Tensor(y).to(device)\n",
    "        trainy = trainy.transpose(1, 3)\n",
    "        metrics = engine.train(trainx, trainy[:,0,:,:])\n",
    "        train_loss.append(metrics[0])\n",
    "        train_mape.append(metrics[1])\n",
    "        train_rmse.append(metrics[2])\n",
    "        if iter % args.print_every == 0 :\n",
    "            log = 'Iter: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}'\n",
    "            print(log.format(iter, train_loss[-1], train_mape[-1], train_rmse[-1]),flush=True)\n",
    "    t2 = time.time()\n",
    "    train_time.append(t2-t1)\n",
    "    #validation\n",
    "    valid_loss = []\n",
    "    valid_mape = []\n",
    "    valid_rmse = []\n",
    "\n",
    "\n",
    "    s1 = time.time()\n",
    "    for iter, (x, y) in enumerate(dataloader['val_loader'].get_iterator()):\n",
    "        testx = torch.Tensor(x).to(device)\n",
    "        testx = testx.transpose(1, 3)\n",
    "        testy = torch.Tensor(y).to(device)\n",
    "        testy = testy.transpose(1, 3)\n",
    "        metrics = engine.eval(testx, testy[:,0,:,:])\n",
    "        valid_loss.append(metrics[0])\n",
    "        valid_mape.append(metrics[1])\n",
    "        valid_rmse.append(metrics[2])\n",
    "    s2 = time.time()\n",
    "    log = 'Epoch: {:03d}, Inference Time: {:.4f} secs'\n",
    "    print(log.format(i,(s2-s1)))\n",
    "    val_time.append(s2-s1)\n",
    "    mtrain_loss = np.mean(train_loss)\n",
    "    mtrain_mape = np.mean(train_mape)\n",
    "    mtrain_rmse = np.mean(train_rmse)\n",
    "\n",
    "    mvalid_loss = np.mean(valid_loss)\n",
    "    mvalid_mape = np.mean(valid_mape)\n",
    "    mvalid_rmse = np.mean(valid_rmse)\n",
    "    his_loss.append(mvalid_loss)\n",
    "    his_loss_train.append(mtrain_loss)\n",
    "\n",
    "    log = 'Epoch: {:03d}, Train Loss: {:.4f}, Train MAPE: {:.4f}, Train RMSE: {:.4f}, Valid Loss: {:.4f}, Valid MAPE: {:.4f}, Valid RMSE: {:.4f}, Training Time: {:.4f}/epoch'\n",
    "    print(log.format(i, mtrain_loss, mtrain_mape, mtrain_rmse, mvalid_loss, mvalid_mape, mvalid_rmse, (t2 - t1)),flush=True)\n",
    "    torch.save(engine.model.state_dict(), args.save+\"_epoch_\"+str(i)+\"_\"+str(round(mvalid_loss,2))+\".pth\")\n",
    "print(\"Average Training Time: {:.4f} secs/epoch\".format(np.mean(train_time)))\n",
    "print(\"Average Inference Time: {:.4f} secs\".format(np.mean(val_time)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "#bestid = np.argmin(his_loss)\n",
    "bestid = np.argmin(his_loss[20:])+20\n",
    "engine.model.load_state_dict(torch.load(args.save+\"_epoch_\"+str(bestid+1)+\"_\"+str(round(his_loss[bestid],2))+\".pth\"))\n",
    "\n",
    "\n",
    "outputs = []\n",
    "realy = torch.Tensor(dataloader['y_test']).to(device)\n",
    "realy = realy.transpose(1,3)[:,0,:,:]\n",
    "\n",
    "for iter, (x, y) in enumerate(dataloader['test_loader'].get_iterator()):\n",
    "    testx = torch.Tensor(x).to(device)\n",
    "    testx = testx.transpose(1,3)\n",
    "    with torch.no_grad():\n",
    "        preds, embs = engine.model(testx)\n",
    "        preds = preds.transpose(1,3)\n",
    "    outputs.append(preds.squeeze())\n",
    "\n",
    "yhat = torch.cat(outputs,dim=0)\n",
    "yhat = yhat[:realy.size(0),...]\n",
    "\n",
    "\n",
    "print(\"Training finished\")\n",
    "print(\"The valid loss on best model is\", str(round(his_loss[bestid],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter, (x, y) in enumerate(dataloader['test_loader'].get_iterator()):\n",
    "    testx = torch.Tensor(x).to(device)\n",
    "    testx = testx.transpose(1,3)\n",
    "    with torch.no_grad():\n",
    "        preds, embs = engine.model(testx)\n",
    "        preds = preds.transpose(1,3)\n",
    "    outputs.append(preds.squeeze())\n",
    "\n",
    "yhat = torch.cat(outputs,dim=0)\n",
    "yhat = yhat[:realy.size(0),...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training finished\")\n",
    "print(\"The valid loss on best model is\", str(round(his_loss[bestid],4)))\n",
    "\n",
    "amae = []\n",
    "amape = []\n",
    "armse = []\n",
    "\n",
    "pred = scaler.inverse_transform(yhat[:,:])\n",
    "real = realy[:,:,0]\n",
    "metrics = util.metric(pred,real)\n",
    "log = 'Evaluate best model on test data for horizon {:d}, Test MAE: {:.4f}, Test MAPE: {:.4f}, Test RMSE: {:.4f}'\n",
    "print(log.format(i+1, metrics[0], metrics[1], metrics[2]))\n",
    "amae.append(metrics[0])\n",
    "amape.append(metrics[1])\n",
    "armse.append(metrics[2])\n",
    "\n",
    "log = 'On average over 12 horizons, Test MAE: {:.4f}, Test MAPE: {:.4f}, Test RMSE: {:.4f}'\n",
    "print(log.format(np.mean(amae),np.mean(amape),np.mean(armse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(engine.model.state_dict(), args.save+\"_exp\"+str(args.expid)+\"_best_\"+str(round(his_loss[bestid],2))+\".pth\")\n",
    "print('Best model saved at: ' + args.save+\"_exp\"+str(args.expid)+\"_best_\"+str(round(his_loss[bestid],2))+\".pth\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
